---
title: "Statistical Relationships"
subtitle: "Correlation"
author: "Andrew Rate"
date: "`r Sys.Date()`"
output: html_document
---

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-table-making-pkgs-etc, include=FALSE}
library(flextable)
library(png)

set_flextable_defaults(theme_fun = "theme_zebra", 
                       font.size = 9, fonts_ignore = TRUE)
BorderDk <- officer::fp_border(color = "#B5C3DF", style = "solid", width = 1)
BorderLt <- officer::fp_border(color = "#FFFFFF", style = "solid", width = 1)

addImg <- function(obj, x = NULL, y = NULL, width = NULL, interpolate = TRUE){
  if(is.null(x) | is.null(y) | is.null(width)){stop("Must provide args 'x', 'y', and 'width'")}
  USR <- par()$usr ; PIN <- par()$pin ; DIM <- dim(obj) ; ARp <- DIM[1]/DIM[2]
  WIDi <- width/(USR[2]-USR[1])*PIN[1] ;   HEIi <- WIDi * ARp 
  HEIu <- HEIi/PIN[2]*(USR[4]-USR[3]) 
  rasterImage(image = obj, xleft = x-(width/2), xright = x+(width/2),
            ybottom = y-(HEIu/2), ytop = y+(HEIu/2), interpolate = interpolate)
}
```

```{r load-necessary-packages, include=FALSE}
# load packages we need
library(car)
library(viridis)
library(psych)
library(corrplot)
library(reshape2)
```

<div style="border: 2px solid #039; background-color:#e8e8e8; padding: 8px;">
Using the R code file provided and the `sv2017` dataset:

- Run different correlation methods (Pearsonâ€™s, Spearmanâ€™s) on untransformed and transformed variables,
- Generate a correlation matrix (or two),
    - Try making a correlation heatmap,
- Try the additional suggestions at the end of the R code file, and
- Make sure you understand the concepts for all the statistical methods you use, and can interpret all the output!

If you need to install packages like `car` or `psych` this should be done first, either:

a. from the RStudio packages tab;
b. running some code like the example below for each package you need to 
install.<br> &nbsp; &nbsp; `install.packages("psych")`

<iframe width="360" height="240" src="https://echo360.net.au/media/f444fd3e-9d67-468b-aeb2-c2b27ca26c5e/public?autoplay=false&amp;automute=false" title="E3361+Correl+Regr.mp4" allowfullscreen="allowfullscreen" webkitallowfullscreen="webkitallowfullscreen" mozallowfullscreen="mozallowfullscreen"></iframe>

<a href="https://raw.githubusercontent.com/Ratey-AtUWA/Learn-R/main/sv2017_original.csv"><span style="font-size: 12pt;">ðŸ’¾&nbsp;The sv2017 dataset in csv (comma-separated) format</span></a><br />
<a href="https://lms.uwa.edu.au/bbcswebdav/pid-3088955-dt-content-rid-40243114_1/xid-40243114_1"><span style="font-size: 12pt;">ðŸ”£&nbsp;R code file for this workshop session</span></a>

</div>

<p>&nbsp;</p>

## Basic correlation analyses

```{r load necessary packages, eval=FALSE}
# load packages we need
library(car)        # for various plotting and regression-specific functions
library(viridis)    # colour palettes for improved graph readability
library(psych)      # utility functions
library(RcmdrMisc)  # utility functions
library(corrplot)   # plotting of correlation heatmaps
```

Let's look at the relationship between Cd and Zn in the Smiths Lake and Charles 
Veryard Reserves data from 2017. <br>
For Pearson's correlation we need variables with normal distributions, so first 
log~10~-transform Cd and Zn...

```{r transform variables, results='hold'}
git <- "https://raw.githubusercontent.com/Ratey-AtUWA/Learn-R/main/"
sv2017 <- read.csv(paste0(git,"sv2017_original.csv"), stringsAsFactors = TRUE)
sv2017$Cd.log <- log10(sv2017$Cd)
sv2017$Zn.log <- log10(sv2017$Zn)
```

...and test if the distributions are normal. Remember that the null hypothesis 
for the Shapiro-Wilk test is that "*the distribution of the variable of * 
*interest is not different from a normal distribution*". So, if the P-value
&ge;&nbsp;0.05, we can't reject the null and therefore our variable is normally 
distributed.

```{r test for normality, results='hold'}
shapiro.test(sv2017$Cd)
shapiro.test(sv2017$Cd.log)
shapiro.test(sv2017$Zn)
shapiro.test(sv2017$Zn.log)
```

<p>&nbsp;</p>

### Pearson's r changes with transformation

```{r pearson correlations, results='hold'}
cor.test(sv2017$Cd, sv2017$Zn, alternative="two.sided", 
         method="pearson") # is Pearson valid?
cor.test(sv2017$Cd.log, sv2017$Zn.log, alternative="two.sided", 
         method="pearson") # is Pearson valid?
```

For the Pearson correlation we get a `t` statistic and associated degrees of
freedom (`df`), for which there is a `p-value`. The null hypothesis is that 
there is "*no relationship between the two variables*", which we can reject if 
p &le; 0.05. We also get a 95% confidence interval for the correlation 
coefficient, and finally an estimate of Pearson's r (`cor`).

A Spearman coefficient doesn't change with transformation, since it is
calculated from the ranks (ordering) of each variable.

```{r message=FALSE, warning=FALSE, results='hold'}
cor.test(sv2017$Cd, sv2017$Zn, alternative="two.sided", 
         method="spearman") # can we use method="pearson"?
cor.test(sv2017$Cd.log, sv2017$Zn.log, alternative="two.sided", 
         method="spearman") # can we use method="pearson"?
```

For the Spearman correlation we get an S statistic and associated p-value. The
null hypothesis is that there is no relationship between the two variables,
which we can reject if p &le; 0.05. We also get an estimate of Spearman's rho
(analogous to Pearson's r).

<p>&nbsp;</p>

## Simple scatterplot by groups

We're using base **R** to plot - you might like to try using 
[`scatterplot()` from the car package](plots.html) or 
[the `ggplot2` package](plots-GG.html).

Before plotting, we first set our desired colour palette (optional) and graphics
parameters (optional, but useful and recommended!). We end up with the plot in 
Figure 1 which helps us interpret the correlation coefficient.

```{r Cd-Zn-scatter, fig.height=4, fig.width=4, fig.cap="Figure 1: Scatterplot showing the relationship between Cd and Zn, with observations identified by Type but showing the regression line for all data independent of grouping.", results='hold'}
palette(c("black",viridis::plasma(8)[2:7],"white"))
par(mfrow=c(1,1), mar=c(3,3,1,1), mgp=c(1.5,0.3,0), oma=c(0,0,0,0), tcl=0.2,
    lend="square", ljoin="mitre", font.lab=2)

# . . . then plot the data
with(sv2017, plot(Cd~Zn, col=c(5,3,1)[Type], log="xy", 
     pch=c(16,1,15)[Type], lwd=c(1,2,1)[Type], 
     cex=c(1.2,1,1)[Type], xlab="Zn (mg/kg)", ylab="Cd (mg/kg)"))
abline(lm(sv2017$Cd.log~sv2017$Zn.log)) # line of best fit using lm() function
legend("topleft", legend=c(levels(sv2017$Type),"Best-fit line ignoring Type"), 
       col=c(5,3,1,1), pch=c(16,1,15,NA), pt.lwd=c(1,2,1), lwd = c(NA,NA,NA,1), 
       pt.cex=c(1.2,1,1), bty="n", y.intersp = 0.75,
       title=expression(italic("Sample Type")))
```

<p>&nbsp;</p>

<hr style="height: 2px; background-color: #660F00;" />

> "The invalid assumption that correlation implies cause is probably among the
  two or three most serious and common errors of human reasoning."
>
> --- [Stephen Jay Gould](https://en.wikipedia.org/wiki/Stephen_Jay_Gould){target="_blank"}

<hr style="height: 2px; background-color: #660F00;" />

# Correlation matrix

We'll generate Spearman correlation matrices in these examples, but it's easy to change the code to generate Pearson (or other) correlation matrices. <br>
The p-values give the probability of the observed relationship if the null hypothesis (*i.e*. no relationship) is true.

```{r correlation-matrix-using-psych, results='hold'}
library(psych) # needed for corTest() function
corr_table <- corTest(sv2017[c("pH","Al","Ca","Fe","Cd","Pb","Zn")], 
                    method="spearman")
print(corr_table)
```

The output from `psych::corTest()`has three sub-tables:

1. the correlation coefficients for each pair of variables (note symmetry)
2. the number of pairs of observations for each relationship (some observations 
may be missing)
3. the p values for each relationship (raw p-values are reported *below* the diagonal and p-values adjusted for multiple comparisons *above* the diagonal (by default based on Holm's correction.)

[Note that for Pearson correlations we instead use `method="pearson"` in the 
`corTest` function.]

It can be a little tricky to compare raw and adjusted p-values, so we can tidy it up with a bit of code:

```{r just-corrmat-pvals, results='hold'}
corr_pvals <- corr_table$p
corr_pvals[which(corr_pvals==0)] <- NA
cat("----- Adjusted (upper) and raw (lower) p-values for correlations -----\n")
print(round(corr_pvals, 3), na.print = "")
```

As above,the p-values give the probability of the observed relationship if the
null hypothesis (*i.e*. no relationship) is true. Corrections are to reduce the
risk of **Type 1 Errors (false positives)** which is greater when multiple
comparisons are being made. We should **always use the adjusted p-values** to
interpret a correlation matrix.

<p>&nbsp;</p>

Since a correlation coefficient is a standardised measure of association, we can 
treat it as an 'effect size'. <br>
Cohen (1988) suggested the following categories:

<table width="40%" border="0">
<tbody>
<tr style="background-color: #e0e0e0;">
<td align="center" colspan=3>**Range in r**</td>
<td align="left">**Effect size term**</td>
</tr>
<tr>
<td align="right">0 &lt;</td>
<td align="center">|r|</td>
<td align="left">&le; 0.1</td>
<td align="left">negligible</td>
</tr>
<tr style="background-color: #e0e0e0;">
<td align="right">0.1 &lt;</td>
<td align="center">|r|</td>
<td align="left">&le; 0.3</td>
<td align="left">small</td>
</tr>
<tr>
<td align="right">0.3 &lt;</td>
<td align="center">|r|</td>
<td align="left">&le; 0.5</td>
<td align="left">medium</td>
</tr>
<tr style="background-color: #e0e0e0;">
<td align="right">0.5 &lt;</td>
<td align="center">|r|</td>
<td align="left">&le; 1</td>
<td align="left">large</td>
</tr>
</tbody>
</table>
<span style="font-size:10pt;">|r| means the absolute value of r (*i.e*. ignoring 
whether r is positive or negative)</span>

<p>&nbsp;</p>

## Correlation heatmaps

These are a handy visual way of displaying a correlation matrix. We use the
`corrplot()` function from the **R** package `corrplot`. We'll use some
different data to show a plot with both positive and negative correlations, and
display the text correlation matrix after the heatmap (Figure 2) for reference.

```{r correl-heat, fig.height=6, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 2: Correlation heatmap (Spearman) for selected variables in the 2023 Ashfield Flats sediment data. The numeric correlation matrix is below.", results='hold'}
afs23 <- read.csv(paste0(git,"afs23.csv"), stringsAsFactors = TRUE)
library(corrplot)
cormat1 <- corTest(afs23[,c("pH","EC","Al","Ca","Fe","Cu","Gd","Pb","Zn")], 
                        method="spearman")
print(cormat1$r, digits=2)
corrplot(cormat1$r, 
         method="ellipse", diag=FALSE, addCoef.col = "black", 
         tl.col = 1, tl.cex = 1.2, number.font = 1, cl.cex=1)
```

<p>&nbsp;</p>

[There is also a simpler correlation heatmap function in the `psych` package, 
`cor.plot()`.]

We can also (with a bit of manipulation of the correlation matrix using the
`reshape2` package) make a heatmap in `ggplot2` using `geom_tile()`, as shown 
below in Figure 3. We use the correlation matrix `cormat1` generated above.

```{r gg-cor-heatmap, fig.height=4, fig.width=5, fig.cap="Figure 3: Correlation heatmap (Spearman) drawn using `ggplot2` for selected variables in the 2023 Ashfield Flats sediment data.", message=FALSE, warning=FALSE, paged.print=FALSE}
library(reshape2)
cormelt <- melt(cormat1$r) # convert mtrix to long format
cormelt$value[which(cormelt$value>=0.999999)] <- NA # remove diagonal values=1
# draw the ggplot heatmap
library(ggplot2)
ggplot(data=cormelt, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(na.rm=T, color="white", linewidth=1) + labs(x="", y="") +
  geom_text(aes(label = round(value,2)), size = 4) + 
  scale_fill_gradient2(high="steelblue", mid="white", low="firebrick", 
                       na.value="white", name="r scale") +
  theme_minimal() + theme(aspect.ratio = 1)
```

<p>&nbsp;</p>

## Scatter plot matrix to check correlation matrix

It's **always** a good idea to plot scatterplots for the relationships we are 
exploring. Scatter (*x-y*) plots can show if:

- a correlation coefficient is unrealistically high due to a small number of outliers, or because there are aligned (or only 2) groups of points
- a correlation coefficient is low, not because of a lack of relationship, but because relationships differ for different groups of points;
- there is a consistent relationship for all groups of observations.

```{r scatter-plot-matrix, fig.height=8, fig.width=8, fig.cap="Figure 4: Scatter plot matrix for selected variables in the 2017 Smiths-Veryard sediment data, with observations and regression lines grouped by sample Type. Scatter plot matrices are a powerful exploratory data analysis tool.", results='hold'}
require(car) # needed for scatterplotMatrix() function
palette(c("black",viridis::plasma(8)[2:7],"white")); carPalette(palette())
scatterplotMatrix(~pH+ log10(Al)+ log10(Ca)+ log10(Cd)+ 
                    log10(Fe)+ log10(Pb)+ log10(Zn) | Type, 
                  smooth=FALSE, ellipse=FALSE, by.groups=TRUE, 
                  col=c(4,2,1), pch=c(16,1,15), cex.lab=1.5, data=sv2017,
                  legend=list(coords="bottomleft"))
```

(See [Figures 18 and 19 in the `ggplot` page](plots-GG.html) if you want to make scatter plot matrices in `ggplot2`.)

<p>&nbsp;</p>

In Figure 4, Pb and Zn are positively related (Pearson's r = 0.77, p<0.0001)
independent of which group (Sediment, Soil, or Street dust) observations are
from. Conversely, the relationship between pH and Cd is weak (r=0.23, adjusted
p=0.32), but there appear to be closer relationships between observations from
individual groups. Finally, the relationship between Al and Ca may be influenced
by a single observation with low Al and high Ca. All issues like this should be
considered when we explore our data more deeply.

The classic example of correlations which need plots to interpret them is the 
somewhat famous **Anscombe's Quartet**. This is shown below in Figure 5:

```{r anscombe, echo=FALSE, fig.height=6, fig.width=7, out.width="70%", fig.cap="Figure 5: Anscombe's Quartet â€“ plots of 4 distinct x-y data sets with regression lines, all having the same slope, intercept, Pearson's r, and RÂ² values. The data are available in R via `datasets::anscombe`.", message=FALSE, warning=FALSE}
par(mar=c(3,3,1,1), mfrow=c(2,2), mgp=c(1.5,0.2,0), tcl=0.25, font.lab=2, cex=1.2)
with(datasets::anscombe, plot(x1,y1, pch=19, xlim=c(4,20), ylim = c(3,13),
                              col="#800000", col.axis="#800000", col.lab="#800000"))
lm1 <- with(datasets::anscombe, lm(y1~x1))
pr <- signif(with(datasets::anscombe, cor(x1,y1)),2)
text(20,5, pos=2, col="#800000",
     labels=paste0("y = ",signif(lm1$coef[2],2),"x + ",signif(lm1$coef[1],2)))
text(20,3.8, pos=2, col="#800000",
     labels=paste0("r = ", pr,"; R\u00B2 = ",signif(summary(lm1)$r.sq,2)))
abline(lm1, lwd = 2, col = "#80000080")
with(datasets::anscombe, plot(x2,y2, pch=19, xlim=c(4,20), ylim = c(3,13),
                              col="#000080", col.axis="#000080", col.lab="#000080"))
lm2 <- with(datasets::anscombe, lm(y2~x2))
pr <- signif(with(datasets::anscombe, cor(x2,y2)),2)
text(20,5, pos=2, col="#000080",
     labels=paste0("y = ",signif(lm2$coef[2],2),"x + ",signif(lm2$coef[1],2)))
text(20,3.8, pos=2, col="#000080",
     labels=paste0("r = ", pr,"; R\u00B2 = ",signif(summary(lm2)$r.sq,2)))
abline(lm2, lwd = 2, col = "#00008080")
with(datasets::anscombe, plot(x3,y3, pch=19, xlim=c(4,20), ylim = c(3,13),
     col="#500090", col.axis="#500090", col.lab="#500090"))
lm3 <- with(datasets::anscombe, lm(y3~x3))
pr <- signif(with(datasets::anscombe, cor(x3,y3)),2)
text(20,5, pos=2, col="#500090",
     labels=paste0("y = ",signif(lm3$coef[2],2),"x + ",signif(lm3$coef[1],2)))
text(20,3.8, pos=2, col="#500090",
     labels=paste0("r = ", pr,"; R\u00B2 = ",signif(summary(lm3)$r.sq,2)))
abline(lm3, lwd = 2, col = "#50009080")
with(datasets::anscombe, plot(x4,y4, pch=19, xlim=c(4,20), ylim = c(3,13),
     col="#005090", col.axis="#005090", col.lab="#005090"))
lm4 <- with(datasets::anscombe, lm(y4~x4))
pr <- signif(with(datasets::anscombe, cor(x4,y4)),2)
text(20,5, pos=2, col="#005090",
     labels=paste0("y = ",signif(lm4$coef[2],2),"x + ",signif(lm4$coef[1],2)))
text(20,3.8, pos=2, col="#005090",
     labels=paste0("r = ", pr,"; R\u00B2 = ",signif(summary(lm4)$r.sq,2)))
abline(lm4, lwd = 2, col = "#00509080")

```

<p>&nbsp;</p>

# References

Cohen, J. 1988. *Statistical Power Analysis for the Behavioral Sciences*, Second Edition. Erlbaum Associates, Hillsdale, NJ, USA.

Reimann, C., Filzmoser, P., Garrett, R.G., Dutter, R., (2008). *Statistical Data Analysis Explained: Applied Environmental Statistics with R*. John Wiley & Sons, Chichester, England (see Chapter 16).

<p>&nbsp;</p>
