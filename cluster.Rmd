---
title: "Cluster Analysis for Compositional Data"
subtitle: "Multivariate analysis"
author: "Andrew Rate"
date: "`r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    code_folding: show
    self_contained: no
    number_sections: no
    smart: no
    toc: true
    toc_depth: 2
    toc_float: true
---

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

# K-means clustering

K-means clustering is an unsupervised classification method, which is a type of
machine learning used when you don't know (or don't want to make assumptions
about) any categories or groups. We are using the same cities land-use dataset
from a previous session, from Hu *et al*. (2021). We do have groupings in these 
data, which are the factors **Type**, **Global**, and **Region**.

```{r load-packages-etc-hidden, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
library(car)
library(cluster)
library(factoextra)
library(ggplot2)
library(reshape2)
library(ggpubr)
library(flextable)

set_flextable_defaults(theme_fun = "theme_booktabs", font.size = 10, 
                       font.family = "Arial", font.color = "black", 
                       fonts_ignore = TRUE)
  BorderDk <- officer::fp_border(color = "#003087", style = "solid", width = 1)
  BorderLt <- officer::fp_border(color = "#ddaa00", style = "solid", width = 1)

palette(c("black","blue","green4","red2","purple","darkcyan",
          "firebrick","grey","grey40","white","transparent"))
```

```{r load-packages-show, message=FALSE, warning=FALSE, eval=FALSE}
library(car)
library(cluster)
library(factoextra)
library(ggplot2)
library(reshape2)
library(ggpubr)
library(flextable)
```

```{r read file and show data, paged.print=FALSE, results='hold'}
# read and check data
cities <- read.csv("cities_Hu_etal_2021.csv", stringsAsFactors = TRUE)
cities$City <- as.character(cities$City)
head(cities)
```

```{r make new Type factor with abbreviated names, include=FALSE, results='hide'}
row.names(cities) <- as.character(cities$City)
cities$sType <- as.character(cities$Type)
cities$sType <- gsub("Compact-Open","CO",cities$sType)
cities$sType <- gsub("Open-Lightweight","OL",cities$sType)
cities$sType <- gsub("Compact","C",cities$sType)
cities$sType <- gsub("Open","O",cities$sType)
cities$sType <- gsub("Industrial","I",cities$sType)
cities$sType <- as.factor(cities$sType)
```

&nbsp;

#### We remove compositional closure by CLR-transformation

```{r clr-transform-data, message=FALSE, warning=FALSE, paged.print=FALSE, results='hold'}
cities_clr <- cities
cities_clr[,2:5] <- t(apply(cities[,2:5], MARGIN = 1,
                           FUN = function(x){log(x) - mean(log(x))}))
head(cities_clr)
```

&nbsp;

The goal of the K-means clustering algorithm is to find a specified number (`K`)
groups based on the data. The algorithm first requires an estimate of the number
of clusters, and there are several ways to do this. The code below, using
functions from the `factoextra` R package tests different values of `K` and
computes the 'total within sum of squares' (WSS) based on the distance of
observations from the 'centroid' (mean) of each cluster, which itself is found
by an iterative procedure. When the decrease in WSS from `K` to `K+1` is
minimal, the algorithm selects that value of `K`.

```{r optimum-clusters-closed, eval=FALSE, fig.height=3, fig.width=3.6, message=FALSE, warning=FALSE, include=FALSE}
data0 <- na.omit(cities[,c("Compact","Open","Lightweight","Industry")])
nclus_clos <- fviz_nbclust(data0, kmeans, method = "wss") +
  geom_vline(xintercept = 3, linetype = 2) +
  labs(title="")
# ggarrange(nclus_clos,nclus_clr,ncol = 2,nrow = 1, 
#           labels = c("(a) closed","(b) open (clr)"))
```

```{r optimum-clusters-open, fig.height=3, fig.width=3.6, out.width='40%', fig.align='center', fig.cap="Figure 1: Estmation of the optimum number of clusters for CLR-transformed urban land-use data.", results='hold'}
#
require(factoextra)
data0 <- na.omit(cities_clr[,c("Compact","Open","Lightweight","Industry")])
(nclus_clr <- fviz_nbclust(data0, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2) +
  labs(title=""))
```

&nbsp;

We have indicated, in Figure 1, five clusters for the cities data, based on
visual identification of a break in the slope of the WSS *vs*. 'Number of
clusters' curve (obviously this is somewhat subjective). In the following
analyses we will assume the same number of clusters (`K` = 5) for K-means
clustering.

```{r kmeans closed, eval=FALSE, include=FALSE, results='hold'}
## Compute K-means clustering for closed data

data0 <- na.omit(cities[,c("sType","Compact","Open","Lightweight","Industry")])
data0[,c("Compact","Open","Lightweight","Industry")] <- 
  scale(data0[,c("Compact","Open","Lightweight","Industry")])
set.seed(123)
cities_clos_kmeans <- kmeans(data0[,2:NCOL(data0)], 4, nstart = 25)
cat("components of output object are:\n")
ls(cities_clos_kmeans)
cat("\nK-means clustering with",length(cities_clos_kmeans$size),
    "clusters of sizes",cities_clos_kmeans$size,"\n\n")
cat("Cluster centers (scaled to z-scores) in K-dimensional space:\n")
cities_clos_kmeans$centers
#
# Applying K-means clustering with 4 clusters to the **closed** cities land-use
# data results in one larger cluster of 25 cities (3) with three smaller clusters
# containing 4-6 cities (1,2, and 4). From the table of cluster centers, we get
# some idea that:
# 
# - Cluster 1 cities have greater proportions of Lightweight land use
# - Cluster 2 cities have greater proportions of Compact land use
# - Cluster 3 cities have similar proportions of all land uses
# - Cluster 4 cities have greater proportions of Open land use
# 
# Interestingly, Lightweight land use does not discriminate between Clusters 2-4,
# since the values of cluster centers 2-4 are the same in the Lightweight
# dimension.
```

```{r eval=FALSE, include=FALSE, results='hold'}
outtable <- data.frame(Cluster = seq(1,length(cities_clos_kmeans$size),1),
                       Cities = rep("nil",length(cities_clos_kmeans$size)))
for (i in 1:length(cities_clos_kmeans$size)){
  outtable[i,1] <- paste("Cluster",i)
  outtable[i,2] <- paste(names(which(cities_clos_kmeans$cluster==i)), 
                         collapse = " ")}
flextable(outtable) |> 
  theme_zebra(odd_header = "#D0E0FF") |> 
  width(j=1:2, width=c(2,5), unit = "in") |> 
  border_inner_h(border=BorderDk, part = "all") |> 
  border_outer(border=BorderDk, part = "all") |>
  border_inner_v(border=BorderLt, part="all") |> 
  set_caption(caption="Table 1: Cities in each K-means cluster from analysis of compsitionally closed data.")
```

&nbsp;

## Compute K-means clustering for open data

```{r kmeans open, results='hold'}
data0 <- na.omit(cities_clr[,c("sType","Compact","Open","Lightweight","Industry")])
data0[,c("Compact","Open","Lightweight","Industry")] <- 
  scale(data0[,c("Compact","Open","Lightweight","Industry")])
set.seed(123)
cities_open_kmeans <- kmeans(data0[,2:NCOL(data0)], 5, nstart = 25)
cat("components of output object are:\n")
ls(cities_open_kmeans)
cat("\nK-means clustering with",length(cities_open_kmeans$size),
    "clusters of sizes",cities_open_kmeans$size,"\n\n")
cat("Cluster centers (scaled to z-scores) in K-dimensional space:\n")
cities_open_kmeans$centers
outtable <- data.frame(Cluster = seq(1,length(cities_open_kmeans$size),1),
                       Cities = rep("nil",length(cities_open_kmeans$size)))
```

&nbsp;

The output object from the `kmeans` function is a list which contains the
information we're interested in: the sum-of-squares between and within clusters
(`betweenss`, `tot.withinss`, `totss`, `withinss`), the location in `K` dimensions of
the centers of the clusters (centers), the assignment of each observation to a
cluster (`cluster`), the number of observations in each cluster (`size`), and the
number of iterations taken to find the solution (`iter`).

&nbsp;

```{r echo=FALSE, results='hold'}
for (i in 1:length(cities_open_kmeans$size)){
  outtable[i,1] <- paste("Cluster",i)
  outtable[i,2] <- paste(names(which(cities_open_kmeans$cluster==i)), 
                         collapse = " ")
  }
flextable(outtable) |>
  width(j=1:2, width=c(5,12), unit = "cm") |> 
  bold(bold=TRUE, part="header") |> color(color="black", part="all") |> 
  set_caption(caption="Table 1: Cities in each K-means cluster from analysis of CLR-transformed cities land-use data.")
```

&nbsp;

Applying K-means clustering with 5 clusters to the **open** cities land-use data
results in one larger cluster of 18 cities (Cluster 1). There are three medium
clusters containing 1-10 cities (2-4), and cluster 5 has a single city. From the
table of cluster centers, we get might conclude that:

- 18 Cluster 1 cities have a more even mix of land uses
- 10 Cluster 2 cities have greater Compact, and lower Open, land uses
- 7 Cluster 3 cities have an even mix of land uses with less compact and more Open than Cluster 1
- 4 Cluster 4 cities have greater Lightweight, and somewhat lesser Open land use
- The 1 Cluster 5 city has greater Industry, and lower Compact, land use

An interesting question to ask is whether the clusters are similar to the
categories we already have in the dataset (`Type`, `Global`, and `Region`). The
following plot includes labelling to help us see the relationship of K-means
clusters to the Type category.

To represent clustering in 3 or more dimensions in a plot, we can use principal
components to reduce the number of dimensions, but still retain information from
all the variables. This is done very nicely by the `fviz_cluster()` function
from the R package `factoextra` (Kassambara and Mundt, 2020).<br /> 
[The output from `fviz_cluster` is a `ggplot`; we don't do it here, but for more
efficient presentation and comparison, we would save the ggplot2 output to
objects, and plot these together using the `ggarrange()` function from the
`ggpubr` R package.]

```{r viz-kmeans-clusters-closed, eval=FALSE, fig.height=5, fig.width=5, message=FALSE, warning=FALSE, include=FALSE}
kmeans_viz_clos <- fviz_cluster(cities_clos_kmeans, data = data0[,2:NCOL(data0)],
             palette = c("#800000", "#E7B800", "#FC4E07","purple2"),
             labelsize=10, main = "",
             ellipse.type = "euclid", # Concentration ellipse
             star.plot = TRUE, # Add segments from centroids to items
             repel = F, # if true avoids label overplotting (slow)
             ggtheme = theme_minimal())
ggarrange(kmeans_viz_clos,kmeans_viz_open,ncol = 2,
          labels = c("(a) closed","(b) open (clr)"))
```

&nbsp;

## Plot kmeans clusters showing factor categories

```{r visualize kmeans clusters compared, fig.align='center', fig.cap="Figure 2: K-means cluster plotm for CLR-transformed (opened) urban land-use data.", fig.height=7, fig.width=7.2, message=FALSE, warning=FALSE, out.width='100%', results='hold'}
row.names(data0) <- paste0(data0$sType,seq(1,NROW(data0)))
fviz_cluster(cities_open_kmeans, data = data0[,2:NCOL(data0)],
             palette = c("#800000", "#E7B800", "#FC4E07","purple2","darkcyan"),
             labelsize=10, main = "",
             ellipse.type = "euclid", # Concentration ellipses
             star.plot = TRUE, # Add segments from centroids to items
             repel = T,        # if true avoids label overplotting (can be slow)
             ggtheme = theme_bw())
```

&nbsp;

From the plot in Figure 2 we can see that, for both the cities land use data,
K-means clustering did not produce clusters that overlap convincingly with the
city Type category in our data. There is some differentiation; for example there
is a cluster (cluster 4) composed only of Open-Lightweight (OL) cities. We
haven't checked the relationship of the other categories (`Global`, `Region`) to
the clusters obtained, but some editing of the code would answer this question
for us!

<hr style="height: 5px; background-color: #5560A4;" />

&nbsp;

# Hierarchical Clustering

Hierarchical clustering is another *unsupervised* classification method,
which groups observations into successively larger clusters (*i.e*.
hierarchically) depending on their *multivariate similarity or
dissimilarity*. The procedure first requires generation of a distance
matrix (we use the `get_dist()` function from the `factoextra` package), 
following which the clustering procedure can begin.

```{r distance matrix closed, message=FALSE, warning=FALSE, echo=FALSE, eval=FALSE, results='hide'}
# Create dissimilarity (distance) matrix for closed data
#
dataHC <- na.omit(cities[,c("City","sType","Compact","Open","Lightweight","Industry")])
row.names(dataHC) <- paste0(dataHC$City, " ", "(", dataHC$sType, seq(1,NROW(dataHC)), ")")
dataHC$City <- NULL
dataHC$sType <- NULL
cities_clos_diss <- get_dist(scale(dataHC), method = "euclidean")
cat("First 8 rows and first 4 columns of distance matrix:\n")
round(as.matrix(cities_clos_diss)[1:8, 1:4], 1)
```

&nbsp;

## Create dissimilarity (distance) matrix

```{r distance matrix open, message=FALSE, warning=FALSE, results='hold'}
dataHC <- na.omit(cities_clr[,c("City","sType","Compact","Open","Lightweight","Industry")])
row.names(dataHC) <- paste0(dataHC$City, " ", "(", dataHC$sType, seq(1,NROW(dataHC)), ")")
dataHC$City <- NULL
dataHC$sType <- NULL
cities_open_diss <- get_dist(scale(dataHC), method = "euclidean")
cat("First 8 rows and first 4 columns of distance matrix:\n")
round(as.matrix(cities_open_diss)[1:8, 1:4], 2)
```

The distance matrix, part of which (the 'top-left' corner) is shown
here, shows the distance in terms of the measurement variables between
any pair of points. In this case the variables are
`Compact`,`Open`,`Lightweight`,and `Industry`, so the resulting
4-dimensional space is hard to visualise, but for *Euclidean distance*
used here it represents the equivalent of straight-line distance between
each observation, each defined by a 4-dimensional vector. There are
several other ways to define distance (or *(dis)similarity*) which are
used in various disciplines, such as Manhattan, Jaccard, Bray-Curtis,
Mahalonobis (and others based on covariance/correlation), or Canberra.
[Try running `?factoextra::dist` and read the information for `method`
for additional information.]

&nbsp;

## Perform hierarchical clustering for closed data

The hierarchical clustering algorithm: **(1)** assigns each observation
to its own cluster, each containing only one observation. So, at the
beginning, the distances (or (dis)similarities) between the clusters is
the same as the distances between the items they contain. **(2)** The
closest pair of clusters is then computed, and this pair is merged into
a single cluster. **(3)** Distances are re-calculated between the new
cluster and each of the old clusters, and steps 2 and 3 are repeated
until all items are clustered into a single cluster. Finally, now the
algorithm has generated a complete hierarchical tree, we can split the
observations into `k` clusters by separating them at the `k-1` longest
branches (for our data, see Figure 3).

Since the clusters are a group of points rather than single points,
there are several ways of combining the observations into clusters &ndash;
run `?stats::hclust` and see under 'method' for more information.

&nbsp;

### Make Hierarchical clustering object

```{r make HClust objects, message=FALSE, warning=FALSE, results='hold', echo=-1}
# cities_clos_hc <- hclust(cities_clos_diss, method = "complete")
cities_open_hc <- hclust(cities_open_diss, method = "complete")
```

&nbsp;

### Assess the validity of the cluster tree

```{r verify cluster tree closed, results='hold', echo=FALSE, eval=FALSE, results='hide'}
# **For closed data:**
# 
cities_clos_coph <- cophenetic(cities_clos_hc)
cat("Correlation coefficient r =",cor(cities_clos_diss,cities_clos_coph),"\n")
cat("\nRule-of-thumb:\n",
 "Cluster tree represents actual distance matrix accurately enough if r>0.75\n")
```

```{r verify cluster tree open, results='hold'}
# For open data:
cities_open_coph <- cophenetic(cities_open_hc)
cat("Correlation coefficient r =",cor(cities_open_diss,cities_open_coph),"\n")
cat("\nRule-of-thumb:\n",
 "Cluster tree represents actual distance matrix accurately enough if r > 0.75\n")
```

&nbsp;

The *cophenetic distance* may be considered a 'back calculation' of the distance
matrix based on the dendrogram (run `?cophenetic` for more details). We then
calculate a correlation coefficient between the actual distance matrix and the
cophenetic distance matrix. If the correlation is great enough (nominally &gt;
0.75), we assume that the dendrogram adequately represents our data. (If the
actual *vs*. cophenetic correlation is too low, we may want to choose another
distance measure, or a different hierarchical clustering algorithm.) Our results
show that the dendrogram for our data is an adequate representation of the
applicable distance matrix.

```{r hierarch-clust-plot, fig.align='center', fig.cap="Figure 3: Hierarchical cluster dendrogram for open (CLR-transformed) urban land-use data.", fig.height=8, fig.width=10, message=FALSE, warning=FALSE, results='hold', echo=1:3}
plot(cities_open_hc, cex=0.8, main="")
abline(h = c(2.2,2.8,4.1), col = c("blue2","purple","red3"), lty = c(2,5,1))
text(c(38,38,38),c(2.2,2.8,4.1), pos=3, 
     col=c("blue2","purple","red3"), offset = 0.2, 
     labels=c("Cut for 5 clusters","Cut for 4 clusters","Cut for 3 clusters"))
# par(mfrow=c(2,1), mgp = c(1.7,0.3,0), tcl = 0.25)
# plot(cities_clos_hc, cex=0.8, main=""); mtext("(a)", adj = 0.05)
# abline(h = c(760,850,1050), col = c("blue2","purple","red3"), lty = c(2,5,1))
# # text(3,870, pos=3, labels="Cut for 5 clusters", col = "blue2", offset = 0.2)
# text(c(30,8,8),c(760,850,1050), pos=c(1,3,3), offset = 0.2, 
#      col = c("blue2","purple","red3"), 
#      labels=c("Cut for 5 clusters","Cut for 4 clusters","Cut for 3 clusters"))
```

&nbsp;

The plot in Figure 3 is a hierarchical clustering 'tree', or *dendrogram*. We
can see that the lowest order clusters, joined above the variable names, contain
two observations (cities, in this dataset), reflecting the first iteration of
Step 2 in the clustering algorithm described above. As we look higher in the
dendrogram, we see additional observations or clusters grouped together, until
at the highest level a single grouping exists. 'Cutting' the dendrogram at
different heights will result in different numbers of clusters; the lower the
cut level, the more clusters we will have. For the cities data we have shown
cuts in Figure 3 resulting in 3, 4, or 5 clusters. There are numerous ways to
decide where to make the cut; for some of these, see 
[Kassambara (2018)](https://shorturl.at/cntxH){target="_blank"}.

&nbsp;

## Find the optimum number of clusters

We did a similar analysis on the same dataset (cities land-use) for K-means
clustering, estimating the optimum number of clusters using the 'weighted
sum-of-squares' (`wss`) method. In Figure 4 we estimate the best cluster numbers
using another common method, the *silhouette* method.

```{r optimum-h-clusters-closed, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
data0 <- na.omit(cities[,c("Compact","Open","Lightweight","Industry")])
nclus_clos <- fviz_nbclust(data0, hcut, method = "silhouette", verbose = F) +
  labs(title="")
ggarrange(nclus_clos,nclus_clr,ncol = 2,nrow = 1, 
          labels = c("(a) closed","(b) open (clr)"))
```

```{r optimum-h-clusters, fig.height=3, fig.width=3.5, out.width='45%', fig.align='center', fig.cap="Figure 4: Estmation of the optimum number of clusters by the silhouette method for open (CLR-transformed) urban land-use data.", results='hold'}
#
require(factoextra)
data0 <- na.omit(cities_clr[,c("Compact","Open","Lightweight","Industry")])
fviz_nbclust(data0, hcut, method="silhouette", verbose=F) +
  labs(title="")
```

&nbsp;

We can also assess optimum cluster numbers using the **NbClust()** function in
the 'NbClust' package:

```{r NbClust package, results='hold', cols.print=120}
cat("Open CLR data:\n")
NbClust::NbClust(data=cities_clr[,c("Compact","Open","Lightweight","Industry")], 
                 distance = "euclidean", min.nc = 2, max.nc = 15, 
                 method = "complete", index = "silhouette")
```

&nbsp;

**Note** that there are **very many combinations** of making a distance matrix
and clustering methods! See `?NbClust::NbClust` for a list of possibilities.

&nbsp;

## Cut dendrogram into clusters

This was illustrated in Figure 3 above, and we now
investigate the clustering in more detail, assuming 4 clusters.

```{r cut dendrogram open, paged.print=FALSE, results='hold'}
cities_open_grp <- cutree(cities_open_hc, k = 4)
tccg <- data.frame(table(cities_open_grp))
outtable <- data.frame(Cluster = seq(1,nlevels(as.factor(cities_open_grp))),
                       Cities = rep("nil",nlevels(as.factor(cities_open_grp))),
                       Freq = rep(0,nlevels(as.factor((cities_open_grp)))))
for (i in 1:nlevels(as.factor(cities_open_grp))) {
  outtable[i,1] <- paste("Cluster",i)
  outtable[i,2] <- paste(names(which(cities_open_grp==i)), 
                         collapse = " ")
  outtable[i,3] <- as.numeric(tccg[i,2])}
flextable(outtable) |> 
  theme_zebra(odd_header = "#D0E0FF") |> 
  border_outer(border=BorderDk, part = "all") |> 
  width(j=1:3, width=c(2.5,12.5,2.5), unit = "cm") |> 
  set_caption(caption="Table 2: Cities in each hierarchical cluster from analysis of open (CLR-transformed) data.")
```

&nbsp;

The decision to cut the dendrograms into 4 clusters (*i.e*. at a level
which intersects exactly 4 'branches') is somewhat subjective, but could
be based on other information such as K-means clustering, PCA, or
pre-existing knowledge of the data.

A diagram comparing the clusters in cut dendrograms for closed and open
data is shown below, in Figure 5.

&nbsp;

## Plot dendrograms with cuts

```{r plot cut dendrogram open, fig.height=8, fig.width=10, message=FALSE, warning=FALSE, fig.align='center', fig.cap="Figure 5: Hierarchical cluster dendrograms for CLR-transformed (open) urban land-use data, showing clusters within dashed rectangles.", results='hold'}
fviz_dend(cities_open_hc, k = 4, # Cut in five groups
          main = "", ylab="", cex = 0.7, # label size
          k_colors = c("gray40","red3", "blue2", "purple"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE, # Add rectangle around groups
          labels_track_height = 10 # adjust low margin for long labels
)
```

&nbsp;

This session has shown us how to apply two unsupervised classification
methods, **K-means clustering** and **hierarchical clustering**, to a 
compositional dataset containing cities land-use data.

&nbsp;

## References and R Packages

Hu, J., Wang, Y., Taubenb√∂ck, H., Zhu, X.X. (2021). Land consumption in cities: A comparative study across the globe. *Cities*, **113**: 103163, [https://doi.org/10.1016/j.cities.2021.103163](https://doi.org/10.1016/j.cities.2021.103163){target="_blank"}.

Kassambara, A. (2018). *Determining The Optimal Number Of Clusters: 3
Must* *Know Methods*. Datanovia, Montpellier, France.
([https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/){target="_blank"})

Kassambara, A. and Mundt, F. (2020). *factoextra: Extract and Visualize the Results of Multivariate Data Analyses*. R package version 1.0.7. [https://CRAN.R-project.org/package=factoextra](https://CRAN.R-project.org/package=factoextra){target="_blank"}

Maechler, M., Rousseeuw, P., Struyf, A., Hubert, M., Hornik, K.(2021). *cluster: Cluster Analysis Basics and Extensions*. R package version 2.1.2. [https://CRAN.R-project.org/package=cluster](https://CRAN.R-project.org/package=cluster){target="_blank"}

Reimann, C., Filzmoser, P., Garrett, R. G., & Dutter, R. (2008). *Statistical Data Analysis Explained: Applied Environmental Statistics with R* (First ed.). John Wiley & Sons, Chichester, UK.

Venables, W. N. & Ripley, B. D. (2002) *Modern Applied Statistics with S* (**MASS**). Fourth Edition. Springer, New York. ISBN 0-387-95457-0. [http://www.stats.ox.ac.uk/pub/MASS4/](http://www.stats.ox.ac.uk/pub/MASS4/){target="_blank"}

Wickham, H. (2019). *stringr: Simple, Consistent Wrappers for Common String Operations*. R package version 1.4.0. [https://CRAN.R-project.org/package=stringr](https://CRAN.R-project.org/package=stringr){target="_blank"}
