---
title: "Statistical Relationships"
subtitle: "Simple linear regression"
author: "Andrew Rate"
date: "`r Sys.Date()`"
output: html_document
---

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-packages-etc, include=FALSE}
library(flextable)
library(viridis)
library(png)
library(car)
library(psych)
library(lmtest)
library(effects)

set_flextable_defaults(theme_fun = "theme_zebra", 
                       font.size = 9, fonts_ignore = TRUE)
BorderDk <- officer::fp_border(color = "#B5C3DF", style = "solid", width = 1)
BorderLt <- officer::fp_border(color = "#FFFFFF", style = "solid", width = 1)

addImg <- function(obj, x = NULL, y = NULL, width = NULL, interpolate = TRUE){
  if(is.null(x) | is.null(y) | is.null(width)){stop("Must provide args 'x', 'y', and 'width'")}
  USR <- par()$usr ; PIN <- par()$pin ; DIM <- dim(obj) ; ARp <- DIM[1]/DIM[2]
  WIDi <- width/(USR[2]-USR[1])*PIN[1] ;   HEIi <- WIDi * ARp 
  HEIu <- HEIi/PIN[2]*(USR[4]-USR[3]) 
  rasterImage(image = obj, xleft = x-(width/2), xright = x+(width/2),
            ybottom = y-(HEIu/2), ytop = y+(HEIu/2), interpolate = interpolate)
}
```

<div style="border: 2px solid #039; background-color:#e8e8e8; padding: 8px;">
Using the R code file provided and the `afs1923` dataset:

- Run a simple regression model,
- Assess whether or not a regression model meets the statistical assumptions,
- Try the additional suggestions at the end of the R code file, and
- Make sure you understand the concepts for all the statistical methods you use, and can interpret all the output!

If you need to install packages like `car`, `lmtest`, and `effects`
this should be done first, either:

a. from the RStudio packages tab;
b. running some code like the example below for each package you need to 
install.<br> &nbsp; &nbsp; `install.packages("psych")`

<span style="font-size: 12pt;"><iframe width="360" height="240" src="https://echo360.net.au/media/f444fd3e-9d67-468b-aeb2-c2b27ca26c5e/public?autoplay=false&amp;automute=false" title="E3361+Correl+Regr.mp4" allowfullscreen="allowfullscreen" webkitallowfullscreen="webkitallowfullscreen" mozallowfullscreen="mozallowfullscreen"></iframe></span>

<p><a href="https://raw.githubusercontent.com/Ratey-AtUWA/Learn-R/main/afs1923.csv"><span style="font-size: 12pt;">ðŸ’¾&nbsp;The afs1923 dataset in csv (comma-separated) format</span></a></p>
<p><a href="https://lms.uwa.edu.au/bbcswebdav/pid-3088955-dt-content-rid-40243114_1/xid-40243114_1"><span style="font-size: 12pt;">ðŸ”£&nbsp;R code file for this workshop session</span></a></p>
</div>

Regression and correlation are often considered together, but they are not the
same thing. **Correlation** measures the strength and direction of a
relationship between two variables. **Regression** is a model in which the
variance in one variable (the "*dependent*" variable) is explained by one, or a
combination of, *predictor* variables (also called "independent" variables). We 
will cover only **linear regression**, where the dependent variable is a linear
function of the predictor variable(s).

Our usual first step is to read the data, and change anything we need to:

```{r}
git <- "https://github.com/Ratey-AtUWA/Learn-R/raw/main/"
afs1923 <- read.csv(paste0(git,"afs1923.csv"), stringsAsFactors = TRUE)
afs1923$Type <- factor(afs1923$Type, levels=c("Drain_Sed","Lake_Sed","Saltmarsh","Other"))
afs1923$Type2 <- 
  factor(afs1923$Type2, 
         levels=c("Drain_Sed","Lake_Sed","Saltmarsh W","Saltmarsh E","Other"))
```

Using the `afs1923` dataset, we will make a **simple linear regression** model
predicting chromium from aluminium.

Before we investigate a regression model, we inspect the scatterplot (Figure 1).

```{r Cr-Al-scatterplot, fig.height=5, fig.width=5, fig.cap="Figure 1: Scatterplot showing relationship between Cr and Al at Ashfield Flats Reserve in 2019-2023. Observations and regression lines are separated by sample Type.", results='hold'}
par(mfrow=c(1,1), mar=c(3,3,1,1), mgp=c(1.5,0.5,0), oma=c(0,0,0,0), tcl=0.2,
    cex=1.2, cex.lab=1.1, cex.axis=1., lend="square", ljoin="mitre", font.lab=2)
with(afs1923, plot(Cr ~ Al, data = afs1923, type="n",
            xlab = "Al (mg/kg)", ylab = "Cr (mg/kg)"))
grid(col="grey", lty=3)
afs1923$Al.log <- log10(afs1923$Al)
afs1923$Cr.log <- log10(afs1923$Cr)
with(afs1923, points(Cr ~ Al, data = afs1923, pch=c(21:25)[Type2], 
            bg = plasma(nlevels(afs1923$Type2))[Type2],
            xlab = "Al (mg/kg)", ylab = "Cr (mg/kg)"))
abline(lm(Cr ~ Al, data=afs1923), lwd=3, col="#80808080")
legend("bottomright", legend=levels(afs1923$Type2), box.col="grey", cex=0.85,
       inset=0.01, pt.bg=plasma(nlevels(afs1923$Type2)), pch=c(21:25),
       title=expression(bold("Sample type")))
```

<p>&nbsp;</p>

*We show the Sample type to confirm that all types follow approximately the same relationship* 
&ndash; remember [checking relationships for correlation in a previous session](correl.html)?

Creating a simple regression model is straightforward in **R**. We use the
`lm()` function to create a linear model object (give it a memorable name), with
the model defined by a formula using the tilde `~` operator which most often
means &lsquo;depends on%rsquo;. In our example we have:

-`Cr&nbsp;~&nbsp;Al` which, in context, means `Cr` as a linear function of `Al`
- The *dependent variable* is `Cr` which is conventionally plotted on the $y$-axis
- The *independent variable* or *predictor* is `Al` which is conventionally plotted on the $x$-axis

We then run a `summary()` of the model object, as this gives the most informative output:

```{r simple regression, results='hold'}
lmCrAlsimple <- lm(Cr ~ Al, data = afs1923)
summary(lmCrAlsimple)
```

The output from a linear model summary in R is quite extensive:

1. `Call`: gives the model we specified (for checking)
2. `Residuals`: some summary statistics for the difference of the model from the measured values &ndash; these differences are called the **residuals**
3. `Coefficients`: a table showing the parameters of the line of best fit, shown by the estimates. The *intercept* of the line is in the first row, and the *slope* labelled by the predictor variable. The other columns in the sub-table give the uncertainty in the parameters (Std.Error), and the null hypothesis p-value (`Pr(>|t|)`) based on a t-statistic for each parameter (against H~0~ that there is no effect of a predictor, *i.e*. the slope = 0)
4. `Signif. codes`: just explains the asterisks *** or ** or *
5. The last block of text contains information on how well the model fits the data. We will focus on the R^2^ (R-squared) value, which is equivalent to the proportion of variance in the dependent variable (`Cr` in this example) which is explained by the predictor (`Al` in this example). We should also note the overall **p-value**, based on the variance ratio F-statistic, which tests H~0~ = no effect of any predictor.

<p>&nbsp;</p>

<div style="border: 2px solid #039; padding: 8px;">
<h2 id="assumptions"> Requirements and Assumptions of [linear] regression</h2>

Ideally, any regression model that we create should fulfil certain assumptions.
There are four main assumptions based on the **residuals** of the regression 
model. The residuals are the differences between the model and reality (*e.g*.
for simple linear regression, the residuals are the the vertical distances
between the line of best fit and the actual points).

```{r show-residuals, echo=FALSE, fig.height=3.6, fig.width=4.2, out.width="40%", fig.cap="Figure 2: Example of simple linear regression with residuals represented by vertical lines showing the deviations of observations from the regression model relationship. Data are from Ashfield Flats sediments 2019-2023, resticted to sample `Type == Other`.", message=FALSE, warning=FALSE}
data0 <- afs1923[which(afs1923$Type2=="Other"),c("Al","Cr")]
row.names(data0) <- NULL
lmCrAl <- lm(Cr ~ Al, data=data0)
a <- as.numeric(lmCrAl$coefficients[1]) ; b <- as.numeric(lmCrAl$coefficients[2])
par(mar=c(3,3,0.2,0.2), mgp=c(1.5,0.2,0), tcl=0.2, font.lab=2, lend="square")
with(data0, plot(Cr ~ Al, pch=4, cex=1.4, lwd=2, ylim=c(8,73),
                 xlab="Al (mg/kg)", ylab="Cr (mg/kg)"))
abline(lmCrAl, col="darkgrey", lwd=2)
for(i in 1:nrow(data0)){
  segments(data0$Al[i],data0$Cr[i],data0$Al[i],a+(data0$Al[i]*b),
           col="#ff0000b0", lwd=3)
}
legend(2000,81, bty="n", legend=c("Simple linear regression","Residuals represented\nby vertical lines"),
       pch=c(NA,"|"), pt.cex=c(NA,2), col=c("darkgrey","#ff0000b0"), lwd=c(2,NA), inset=c(0.01,0.0), y.int=1.6)
```

The assumptions of linear regression are:

1. **Linear relationship**: There is a linear relationship between the independent variable, x, and the dependent variable, y.
2. **Independence**: The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data (*i.e*. no *autocorrelation*).
3. **Homoscedasticity**: The residuals have constant variance at every level of x.
4. **Normality**: The residuals of the model are normally distributed.

We can check these assumptions graphically using *diagnostic plots* (see below), 
or using formal statistical tests, which we do after the diagnostic plots.
</div>

<p>&nbsp;</p>

### Diagnostic plots for Simple linear regression, Cr ~ Al

```{r diagnostic-plots-simple, fig.width=5, fig.height=5, fig.cap="Figure 3: Regression diagnostic plots for Cr predicted from Al using a simple linear regression model without grouping", results='hold'}
par(mfrow=c(2,2), mar=c(3,3,1.5,1))
plot(lmCrAlsimple, col="#003087")
par(mfrow=c(1,1), mar=c(3,3,1,1))
```

The diagnostic plots (Figure 3) are a visual test of some of the 
assumptions of linear regression models, which relate mainly to the residuals.
<br>
[An alternative from the `car` package is `influenceIndexPlot(yourModelName)`].

The **top left** and **bottom left** plots in Figure 3 allow us to assess the
assumption of *homoscedasticity*, that is, the residuals should be of similar
absolute magnitude independent of the value of the dependent variable (actual or
predicted). The top left plot also helps us to decide if the residuals are
*independent*. In both the top left and bottom left plots, residuals should
appear randomly distributed with a near-horizontal smoothed 
(<span style="color:#FF0000;">red</span>) line.

The **top right** plot in Figure 3 is a Q-Q plot which tests another assumption
of regression; that the *residuals should be normally distributed*. The points
should lie along (or close to) the theoretical (dotted) line.

Finally the **bottom left** plot in Figure 3 tests whether any observations have
an unusual influence on the regression statistics (the assumption is that they
*do not*).

<h2 id="diagtests">Formal statistical tests of regression assumptions</h2>

We can test all of the assumptions with formal statistical tests using the 
`car` and `lmtest` **R** packages.

We can use the Shapiro-Wilk test to check the assumption of 
*normally distributed residuals* (H~0~ is that the residuals *are* normally 
distributed):

```{r shapiro-test-resids, results='hold'}
shapiro.test(lmCrAlsimple$residuals) # normality
```

The Breusch-Godfrey test is for residual autocorrelation; H~0~ is that
residuals are not autocorrelated (*i.e*. observations probably *independent*)

```{r bgtest, results='hold'}
require(lmtest)
bgtest(lmCrAlsimple) # autocorrelation (independence)
```

The Breusch-Pagan test is for heteroscedasticity; H~0~ is that residuals are
*homoscedastic* (*i.e*. variance independent of value of variable).

```{r bptest, results='hold'}
bptest(lmCrAlsimple) # homoscedasticity
```

The Rainbow test is to test the assumption of *linearity*; H~0~ is that the
relationship is linear.

```{r rainbow-test, results='hold'}
raintest(lmCrAlsimple) # linearity
```

The `outlierTest()` function in the `car` package implements the Bonferroni
outlier test; H~0~ is that all residuals are from the same population (*i.e*. no
outliers). H~0~ is tested with the **Bonferroni** (NOT unadjusted) p-value. If
no Bonferroni p-value is&le;&nbsp;0.05, so we cannot reject H~0~, the function 
outputs the largest residual(s).

```{r outlier-test, results='hold', paged.print=FALSE}
require(car)
outlierTest(lmCrAlsimple) # influential observations (outliers)
```

Another common measure of whether individual observations are influential in the
regression is *Cook's distance*. We often use a rule-of-thumb that observations
with a Cook's distance&nbsp;&gt;0.5 could be considered influential.

```{r cokks-dist, results='hold', paged.print=FALSE}
summary(cooks.distance(lmCrAlsimple)) # influential observations
```

Even more detail can be obtained using the function `influence.measures()`. This 
produces a lot of output, so we show only part of it here.

```{r influence-measures, results='hold'}
infl.lmCrAl <- influence.measures(lmCrAlsimple)
summary(infl.lmCrAl) # influential observations
```

To summarise:

```{r diagnost-summ, echo=FALSE}
diagsumm <- data.frame(Test=c("Shapiro-Wilk", "Breusch-Godfrey","Breusch-Pagan","Rainbow","Outlier","Cook's Distance","Infuence Measures"),
                       Assessing=c("Are residuals normally distributed?", "Are residuals independent?","Are residuals homoscedastic?","Is the relationship linear?","Are there significant outliers (Bonferroni)?","Are there any potentially influential observations?","Are there any potentially influential observations?"),
                       Results=c("No (qq-plot suggests long tails)", "No, there is autocorrelation","Yes (p<0.05)","Yes (p<0.05)","3 points are possible outliers","No (no values > 0.5)","Yes (17 out of 335 observations)"))
flextable(diagsumm) |> width(width=c(4,8,5), unit = "cm") |> valign(valign = "top") |> padding(padding=2) |> 
  set_caption(caption="Table 1: Summary of regression diagnostic tests for a model predicting Cr from Al in sediments from Ashfield Flats Reserve, based on data from samples collected from 2019-2023.", align_with_table=F, fp_p=fp_par(text.align = "left", padding.bottom = 6))
```

<p>&nbsp;</p>

The summary of the diagnostic tests in Table 1 shows that not all the formal
assumptions of regression are fulfilled by our model. While this means the model
may not be perfectly valid from a statistical perspective, we also recall that
the regression output showed that nearly 80% of the variance in Cr could be
explained by Al. So, from a practical perspective, our simple regression model
could still be used for prediction.

We can show the regression model relationship with two types of interval: a 
*confidence interval*, and a *prediction interval*:

```{r lm-conf-pred-intervals, fig.height=3.2, fig.width=4, fig.cap="Figure 4: Simple linear regression relationship bewteen Cr and Al, showing 95% confidence and prediction intervals. Data are from Ashfield Flats sediments 2019-2023.", message=FALSE, warning=FALSE, echo=FALSE}
newdata <- data.frame(Al=pretty(c(min(afs1923$Al, na.rm=T)-5000, afs1923$Al, max(afs1923$Al, na.rm=T)+5000), 100))
lmCrAlconf <- predict(lmCrAlsimple,newdata, interval = "conf")
lmCrAlpred <- predict(lmCrAlsimple,newdata, interval = "predict")
par(mar=c(2.5,2.5,0.25,0.25), mgp=c(1.3,0.2,0), tcl=0.2, font.lab=2)
with(lmCrAlsimple$model, plot(Cr ~ Al, ylim=c(-10,110), type="n", cex.axis=0.8, cex.lab=0.9))
# lines(newdata$Al, lmCrAlpred[,2], lty=2, col=4)
# lines(newdata$Al, lmCrAlpred[,3], lty=2, col=4)
# lines(newdata$Al, lmCrAlconf[,3], col=2, lwd=2)
# lines(newdata$Al, lmCrAlconf[,2], col=2, lwd=2)
polygon(c(newdata$Al,rev(newdata$Al)), c(lmCrAlpred[,2], rev(lmCrAlpred[,3])), 
        col="#4040ff40", border="transparent")
polygon(c(newdata$Al,rev(newdata$Al)), c(lmCrAlconf[,2], rev(lmCrAlconf[,3])), 
        col="#ff0000a0", border="transparent")
with(lmCrAlsimple$model, points(Cr ~ Al, col="dimgrey"))
abline(lmCrAlsimple, lwd=1.5, col="black")
legend("topleft", bty="n", legend=c("95% confidence", "Prediction"), cex=.9,
       title=expression(italic("Intervals")), 
       col=c("#e00020a0","#4040ff40"), pch=15, pt.cex=1.7, y.int=0.8)
legend("bottomright", bty="n", legend=c("Observations", "Linear regression"), cex=.9,
       col=c("dimgrey","black"), pch=c(1,NA), lty=c(NA,1), lwd=c(NA,1.5), y.int=0.8,
       seg.len = 1.5, pt.cex=c(1,NA))
```

<p>&nbsp;</p>

The intervals in Figure 4 are noticeably different, with the confidence interval 
much narrower than the comparable prediction interval. The key differences are that:

- the **confidence interval** shows the range of *mean* Cr for any given value of Al that would be expected 95% of the time
- the **prediction interval** shows the range of Cr values that *a single additional sample* could take, for different values of Al

The prediction interval, then, is the one we need to consider when predicting an
unknown concentration of Cr from a known concentration of Al in sediment. In 
this example, with R^2^ nearly 0.8, we may not have expected such a large
prediction uncertainty &ndash; but it's good to be aware of the practical 
limitations of regression modelling.

A very practical application of regression prediction is when using **calibration 
curves** in different types of sample analysis. The plots in Figure 5 show why 
your lab instructor nags you to take care with your measurements when preparing 
standards and samples, and demands that your calibration has RÂ² close to 0.9999!

<div class="figure">
![](./images/FRP_calib.png){alt="Figure 5: Calibration curves for filterable reactive phosphate of different quality and having different RÂ² values. The dotted lines show interpolation of the calibration lines at Absorbance = 0.29, and the resulting range of prediction intervals."}

<p class="caption">Figure 5: Calibration curves for filterable reactive phosphate of different quality and having different RÂ² values. The dotted lines show interpolation of the calibration lines at Absorbance = 0.29, and the resulting range of prediction intervals.</p>

</div>

<p>&nbsp;</p>

<div style="border: 2px solid #039; background-color:#e8e8e8; padding: 8px;">
Subsequent sessions will cover grouped and multiple regression. These methods 
may make prediction more accurate by: 

i. allowing regression relationships to differ for different groups in our data (defined by a factor variable) &ndash; *regression by groups*, or
ii. testing a model which has more than one predictor variable for our dependent variable &ndash; *multiple regression*.
</div>

<p>&nbsp;</p>

# References

Fox, J. and Weisberg, S. (2019). *An {R} Companion to Applied Regression, Third Edition*. Thousand Oaks CA: Sage. [https://socialsciences.mcmaster.ca/jfox/Books/Companion/](https://socialsciences.mcmaster.ca/jfox/Books/Companion/){target="_blank"} (**R** package `car`)

Garnier S, Ross N, Rudis R, Camargo AP, Sciaini M, Scherer C (2021). *Rvision &ndash; Colorblind-Friendly Color Maps for R*. R package version 0.6.2. (**R** package `viridis`)

Reimann, C., Filzmoser, P., Garrett, R.G., Dutter, R., (2008). *Statistical Data Analysis Explained: Applied Environmental Statistics with R*. John Wiley & Sons, Chichester, England (see Chapter 16).

Zeileis, A. and Hothorn, T. (2002). Diagnostic Checking in Regression Relationships. *R News* **2**(3), 7-10. [https://CRAN.R-project.org/doc/Rnews/](https://CRAN.R-project.org/doc/Rnews/){target="_blank"} (**R** package `lmtest`)

<p>&nbsp;</p>
